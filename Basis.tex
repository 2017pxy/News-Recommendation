\documentclass{article}
\usepackage{dirtree}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage[a4paper]{geometry} 
\usepackage{amsmath,amsthm,mathtools,amssymb}
\usepackage{mathtools}
\usepackage{diagbox}
\usepackage{multirow,makecell}
\usepackage{float}
\usepackage{url}
\usepackage[nottoc]{tocbibind}
\usepackage{float}
\newcommand{\refe}[1]{Eq.\ref{#1}}
\newcommand{\reft}[1]{Theory.\ref{#1}\ }
\newcommand{\reff}[1]{图\ref{#1}\ }
\newtheorem{theorem}{Theory}[section]
\geometry{bottom=2cm,left=1cm,right=1cm}

%插入代码样式设置
\lstset{
 columns=fixed,
 numbers=left,                                        % 在左侧显示行号
 numberstyle=\tiny\color{gray},                       % 设定行号格式
 basicstyle=\small\ttfamily,
 frame=none,                                          % 不显示背景边框
 backgroundcolor=\color[RGB]{245,245,244},            % 设定背景颜色
 keywordstyle=\color[RGB]{40,40,255},                 % 设定关键字颜色
 numberstyle=\footnotesize\color{darkgray},           
 commentstyle=\color{gray}\ttfamily,                  % 设置代码注释的格式
 stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},   % 设置字符串格式
 showstringspaces=false,
 breaklines=true,
 language=python
}

\title{Learning Machine Learning}
\author{Pt}
\begin{document}
    \maketitle
    \tableofcontents
    \clearpage

    \section{Glossary}
    \subsection{Odds and Logit}
    In \emph{Bi-Classification Problem}, the probability of $label=1$ divided by the probability of $label=0$ is called \textbf{Odds}.
    \begin{align*}
        y =& P(label=1)\\
        odds =& \frac{y}{1-y}
    \end{align*}
    Further, take the logarithm of both sides, we got \textbf{Log Odds/Logit:}
    \begin{equation*}
        logit = \ln\frac{y}{1-y}
    \end{equation*}
    \section{Loss Functions and Objective Functions}
    \subsection{Minimum Square Loss (MSE)}

    \subsection{Cross-Entropy/Log Loss}
    For \emph{Bi-classification Problem}. Given that $x_i$ is one of the sample training data, $y_i$ is the corresponding label, then
    \begin{align*}
        \hat{y_i} =&\sigma(h(x_i| \theta)) \in \mathbb{R}\\
        \mathcal{L}(y_i,\hat{y_i}) =& \begin{cases}
            -\log(\hat{y_i}) & y_i = 1\\
            -\log(1-\hat{y_i}) & y_i = 0
        \end{cases}
    \end{align*}
    Compress into one equation, then
    \begin{gather*}
        \mathcal{L}(y_i,\hat{y_i}) = -[y_i *\log(\hat{y_i}) + (1-y_i)*\log(1-\hat{y_i})]
    \end{gather*}
    More generally, for \emph{Multi-Classification Problem}, given that \begin{itemize}
        \item $x_i$ is one of the sample training data, which will be classified into one of $k$ categories,
        \item $y_i \in \mathbb{R}^k$ is the \textbf{One-Hot Representation} of the corresponding label
    \end{itemize}
    the \textbf{Cross-Entropy Loss} is
    \begin{align*}
        \hat{y_i} =&  \ softmax(h(x_i| \theta)) \in \mathbb{R}^k\\
        \mathcal{L}(y_i,\hat{y_i}) =& -\sum_j^n y_i[j] * log_2(\, \hat{y_i}[j])
    \end{align*} 

    \subsection{Maximum (Log-)Likelihood}
    Given the probability of a series of accidents $A_i,i\in{1,2,\dots,k}$ is $P(A_i)$, then we want to \textbf{maximize} the probability
    that all of these accidents happen, thus
    \begin{equation*}
        max\{\prod_{i=0}^{k}P(A_i)\}
    \end{equation*}
    To simplify, we can take the logarithm of both sides, then
    \begin{equation*}
        max\{\ln{\sum_{i=0}^kP(A_i)}\}
    \end{equation*}
    which is namely \textbf{Maximum (Log) Likelihood}. While the \textbf{Loss Function} derived from above is natually:
    \begin{equation*}
        \mathcal{L}(y_i,\hat{y}\mid \theta) = -\ln{\sum_{i=0}^kP(A_i)}
    \end{equation*}
    Which we want to \textbf{Minimize}.
    \section{Probability}
    \subsection{Basis}
    \begin{itemize}
        \item Priori Probability: the probability which can be empirically inferred
        \item Posterior Probability: after A happening, sought the probability of the reason of A
        \item Bayesian Equation
    \end{itemize}
    \subsection{Conditional Indipendent}
    \begin{equation*}
        p(A\mid C) * p(B \mid C) = p(AB\mid C)
    \end{equation*}
    \section{Matrix}
    \subsection{Differentiate/Derivation}
    \begin{align*}
        Y=&\  A\cdot X\cdot B\\
        \frac{\partial  Y}{\partial X} =&\ A^{T} \cdot B^{T}\\
        \frac{\partial  Y}{\partial X^{T}} =&\ B \cdot A 
    \end{align*}
    Another scenario,
    \begin{align*}
        Y =&\ X^{T}\cdot A \cdot X\\
        \frac{\partial Y}{\partial X} =&\ (A + A^{T})\cdot X
    \end{align*}
    \section{Normalization}
    \subsection{Scale}
    Given $p\in \mathbb{R}^k$, where $p$ is the result of \emph{Dot-Product} in \emph{Dot-Product Attention Mechanism}, in order to counteract gradient vanishing in \emph{softmax}, scale $p$ by
    \begin{equation*}
        p_{norm} = \frac{p}{\sqrt{k}}
    \end{equation*}
\end{document}