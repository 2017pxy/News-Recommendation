{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit4c03300bedca44f8b0013abe02048abc",
   "display_name": "Python 3.7.9 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('../')\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from utils.utils import run_eval,train,prepare\n",
    "from models.baseline_cnn import GCAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'mode':'demo',\n",
    "    'name':'baseline-cnn',\n",
    "    'train_embedding':False,\n",
    "    'batch_size':16,\n",
    "    'title_size':20,\n",
    "    'his_size':30,\n",
    "    'npratio':4,\n",
    "    'dropout_p':0.2,\n",
    "    'query_dim':200,\n",
    "    'embedding_dim':300,\n",
    "    'filter_num':400,\n",
    "    'value_dim':16,\n",
    "    'head_num':16,\n",
    "    'epochs':5,\n",
    "    'metrics':'group_auc,ndcg@5,ndcg@10,mean_mrr',\n",
    "    'device':'cuda:1',\n",
    "    'attrs': ['title'],\n",
    "}\n",
    "save_path = 'models/model_params/{}_{}_{}'.format(hparams['name'],hparams['mode'],hparams['epochs']) +'.model'\n",
    "device = torch.device(hparams['device']) if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, loader_train, loader_test, loader_validate = prepare(hparams, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GCAModel(nn.Module):\n",
    "    def __init__(self,hparams,vocab):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cdd_size = (hparams['npratio'] + 1) if hparams['npratio'] > 0 else 1\n",
    "        self.metrics = hparams['metrics']\n",
    "        self.device = torch.device(hparams['device'])\n",
    "        self.embedding = vocab.vectors.to(self.device)\n",
    "\n",
    "        self.batch_size = hparams['batch_size']\n",
    "        self.signal_length = hparams['title_size']\n",
    "        self.his_size = hparams['his_size']\n",
    "\n",
    "        self.dropout_p = hparams['dropout_p']\n",
    "\n",
    "        self.filter_num = hparams['filter_num']\n",
    "        self.embedding_dim = hparams['embedding_dim']\n",
    "       \n",
    "        # elements in the slice along dim will sum up to 1 \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.gumbel_softmax = nn.functional.gumbel_softmax\n",
    "        self.ReLU = nn.ReLU()\n",
    "        self.DropOut = nn.Dropout(p=hparams['dropout_p'])\n",
    "        \n",
    "        self.CNN = nn.Conv1d(in_channels=self.embedding_dim,out_channels=self.filter_num,kernel_size=3,padding=1)\n",
    "        self.SeqCNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=(3,3)),\n",
    "            nn.Conv2d(in_channels=32, out_channels=16, kernel_size=(3,3), padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(3,3), stride=(3,3))\n",
    "        )\n",
    "        \n",
    "        self.learningToRank = nn.Linear(self.repr_dim, 1)\n",
    "        # self.learningToRank = nn.Linear(self.repr_dim * self.his_size, 1)\n",
    "\n",
    "    def _scaled_dp_attention(self,query,key,value):\n",
    "        \"\"\" calculate scaled attended output of values\n",
    "        \n",
    "        Args:\n",
    "            query: tensor of [*, query_num, key_dim]\n",
    "            key: tensor of [batch_size, *, key_num, key_dim]\n",
    "            value: tensor of [batch_size, *, key_num, value_dim]\n",
    "        \n",
    "        Returns:\n",
    "            attn_output: tensor of [batch_size, *, query_num, value_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # make sure dimension matches\n",
    "        assert query.shape[-1] == key.shape[-1]\n",
    "        key = key.transpose(-2,-1)\n",
    "\n",
    "        attn_weights = torch.matmul(query,key)/torch.sqrt(torch.tensor([self.embedding_dim],dtype=torch.float,device=self.device))\n",
    "        attn_weights = self.softmax(attn_weights)\n",
    "        \n",
    "        attn_output = torch.matmul(attn_weights,value)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "    def _self_attention(self,input,head_idx):\n",
    "        \"\"\" apply self attention of head#idx over input tensor\n",
    "        \n",
    "        Args:\n",
    "            input: tensor of [batch_size, *, embedding_dim]\n",
    "            head_idx: interger of attention head index\n",
    "\n",
    "        Returns:\n",
    "            self_attn_output: tensor of [batch_size, *, value_dim]\n",
    "        \"\"\"\n",
    "        query = self.queryProject_words[head_idx](input)\n",
    "\n",
    "        attn_output = self._scaled_dp_attention(query,input,input)\n",
    "        self_attn_output = self.valueProject_words[head_idx](attn_output)\n",
    "\n",
    "        return self_attn_output\n",
    "    \n",
    "    def _word_attention(self, query, key, value):\n",
    "        \"\"\" apply word-level attention\n",
    "\n",
    "        Args:\n",
    "            query: tensor of [1, query_dim]\n",
    "            key: tensor of [batch_size, *, transformer_length, query_dim]\n",
    "            value: tensor of [batch_size, *, transformer_length, repr_dim]\n",
    "\n",
    "        Returns:\n",
    "            attn_output: tensor of [batch_size, *, repr_dim]\n",
    "        \"\"\"\n",
    "        # query = query.expand(key.shape[0], key.shape[1], key.shape[2], 1, self.query_dim)\n",
    "\n",
    "        attn_output = self._scaled_dp_attention(query,key,value).squeeze(dim=-2)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "    def _multi_head_self_attention(self,input):\n",
    "        \"\"\" apply multi-head self attention over input tensor\n",
    "\n",
    "        Args:\n",
    "            input: tensor of [batch_size, *, transformer_length, embedding_dim]\n",
    "        \n",
    "        Returns:\n",
    "            multi_head_self_attn: tensor of [batch_size, *, repr_dim]\n",
    "        \"\"\"\n",
    "        self_attn_outputs = [self._self_attention(input,i) for i in range(self.head_num)]\n",
    "\n",
    "        # project the embedding of each words to query subspace\n",
    "        # keep the original embedding of each words as values\n",
    "        multi_head_self_attn_value = torch.cat(self_attn_outputs,dim=-1)\n",
    "        multi_head_self_attn_key = torch.tanh(self.keyProject_words(multi_head_self_attn_value))\n",
    "\n",
    "        additive_attn_embedding = self._word_attention(self.query_words, multi_head_self_attn_key, multi_head_self_attn_value)\n",
    "        return additive_attn_embedding\n",
    "\n",
    "    def _fusion(self, cdd_news, his_news):\n",
    "        \"\"\" concatenate candidate news title and history news title\n",
    "        \n",
    "        Args:\n",
    "            cdd_news: tensor of [batch_size, cdd_size, signal_length] \n",
    "            his_news: tensor of [batch_size, his_size, signal_length] \n",
    "\n",
    "        Returns:\n",
    "            fusion_news: tensor of [batch_size, cdd_size, his_size, transformer_length]\n",
    "        \"\"\"\n",
    "        fusion_news = torch.zeros((self.batch_size, self.cdd_size, self.his_size, self.transformer_length) ,device=self.device).long()\n",
    "        fusion_news[:,:,:,:self.signal_length] = cdd_news.unsqueeze(dim=2)\n",
    "        fusion_news[:,:,:,(self.signal_length + 1):] = his_news.unsqueeze(dim=1)\n",
    "        # split two news with <PAD>\n",
    "        fusion_news[:,:,:,self.signal_length] = 1\n",
    "        return fusion_news\n",
    "    \n",
    "    def _news_encoder(self,news_batch):\n",
    "        \"\"\" capture local text\n",
    "        \n",
    "        Args:\n",
    "            news_batch: tensor of [batch_size, cdd_size, his_size, transformer_length]\n",
    "        \n",
    "        Returns:\n",
    "            news_emebdding: tensor of [batch_size, cdd_size, his_size, transformer_length, filter_num] \n",
    "        \"\"\"\n",
    "\n",
    "        news_embedding = self.embedding[news_batch].transpose(-2,-1).view(-1,self.embedding_dim,self.transformer_length)\n",
    "        \n",
    "        news_embedding = self.CNN(news_embedding).transpose(-2,-1).view(self.batch_size, self.cdd_size, self.his_size, self.transformer_length, self.filter_num)\n",
    "        news_embedding = self.ReLU(news_embedding)\n",
    "\n",
    "        if self.dropout_p > 0:\n",
    "            news_embedding = self.DropOut(news_embedding)\n",
    "\n",
    "        return news_embedding\n",
    "\n",
    "    def _fusion_transform(self,fusion_news_embedding):\n",
    "        \"\"\" encode fused news into embeddings\n",
    "        \n",
    "        Args:\n",
    "            fusion_news_embedding: tensor of [batch_size, cdd_size, his_size, transformer_length, filter_num]\n",
    "        \n",
    "        Returns:\n",
    "            fusion_repr: tensor of [batch_size, cdd_size, repr_dim]\n",
    "        \"\"\"\n",
    "        fusion_repr = self._multi_head_self_attention(fusion_news_embedding)#.view(self.batch_size, self.cdd_size, -1)\n",
    "        fusion_repr = torch.mean(fusion_repr, dim=-2)\n",
    "\n",
    "        return fusion_repr\n",
    "    \n",
    "    def _click_predictor(self,fusion_repr):\n",
    "        \"\"\" calculate batch of click probability              \n",
    "        Args:\n",
    "            fusion_repr: tensor of [batch_size, cdd_size, repr_dim]\n",
    "        \n",
    "        Returns:\n",
    "            score: tensor of [batch_size, cdd_size]\n",
    "        \"\"\"\n",
    "        score = self.learningToRank(fusion_repr)\n",
    "\n",
    "        if self.cdd_size > 1:\n",
    "            score = nn.functional.log_softmax(score,dim=1)\n",
    "        else:\n",
    "            score = torch.sigmoid(score)\n",
    "        \n",
    "        return score.squeeze()\n",
    "\n",
    "    def forward(self,x):\n",
    "        fusion_news = self._fusion(x['candidate_title'].long().to(self.device), x['clicked_title'].long().to(self.device))\n",
    "        fusion_news_embedding = self._news_encoder(fusion_news)\n",
    "        fusion_repr = self._fusion_transform(fusion_news_embedding)\n",
    "        score_batch = self._click_predictor(fusion_repr)\n",
    "        return score_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training...\n",
      "epoch 0 , step 120 , loss: 1.5559: : 125it [00:43,  2.88it/s]\n",
      "epoch 1 , step 120 , loss: 1.4900: : 125it [00:43,  2.91it/s]\n",
      "epoch 2 , step 120 , loss: 1.4407: : 125it [00:43,  2.91it/s]\n",
      "epoch 3 , step 120 , loss: 1.4026: : 125it [00:42,  2.91it/s]\n",
      "epoch 4 , step 120 , loss: 1.3584: : 125it [00:43,  2.91it/s]\n",
      "0it [00:00, ?it/s]save success!\n",
      "testing...\n",
      "1131it [00:37, 30.26it/s]\n",
      "0it [00:00, ?it/s]evaluation results:{'group_auc': 0.5348, 'ndcg@5': 0.2423, 'ndcg@10': 0.3113, 'mean_mrr': 0.2308}\n",
      "validating...\n",
      "4706it [02:31, 31.02it/s]\n",
      "evaluation results:{'group_auc': 0.7166, 'ndcg@5': 0.3925, 'ndcg@10': 0.453, 'mean_mrr': 0.3656}\n"
     ]
    }
   ],
   "source": [
    "train(gcaModel, hparams, loader_train, loader_test, save_path, loader_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}