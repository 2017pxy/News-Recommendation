{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bitnnconda0d1e803757584bea8110f1ee664506cc",
   "display_name": "Python 3.7.9 64-bit ('nn': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('/home/peitian_zhang/Codes/NR')\n",
    "sys.path.append('/home/peitian_zhang/Codes/NR')\n",
    "\n",
    "import pytz\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "from torchtext.vocab import FastText\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils.MIND import MIND_iter,MIND_map\n",
    "from utils.utils import getLoss,getLabel,constructBasicDict,run_eval,run_train\n",
    "from models.NPA import NPAModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up the *NPA* model\n",
    "\n",
    "### define paths and hyperparameters, load data\n",
    "\n",
    "all these hyper parameters are fixed according to the paper [\\[23\\] Npa Neural news recommendation with personalized attention](https://dl.acm.org/doi/abs/10.1145/3292500.3330665)\n",
    "\n",
    "- *mode*: data to read (*demo*/*small*/*large*)\n",
    "\n",
    "- *batch_size*: size of each minibatch\n",
    "\n",
    "- *title_size*: max word capacity of title\n",
    "\n",
    "- *his_size*: max record capacity of click history\n",
    "\n",
    "- *npratio*: number of negtive sampling\n",
    "\n",
    "- *dropout_p*: probability of dropout layer\n",
    "\n",
    "- *filter_num*: number of kernels in 1D CNN, which is also embedding dimension of news/user\n",
    "\n",
    "- *embedding_dim*: word embedding dimension\n",
    "\n",
    "- *user_dim*: user id embedding dimension\n",
    "\n",
    "- *preference_dim*: user preference embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'mode':'small',\n",
    "    'epochs':10,\n",
    "    'batch_size':100,\n",
    "    'title_size':30,\n",
    "    'his_size':50,   \n",
    "    'npratio':4,\n",
    "    'dropout_p':0.2,\n",
    "    'filter_num':400,\n",
    "    'embedding_dim':300,\n",
    "    'user_dim':50,\n",
    "    'preference_dim':200,\n",
    "    'metrics':'group_auc,ndcg@4,mean_mrr',\n",
    "    'gpu':'cuda:0',\n",
    "    'attrs': ['title']\n",
    "}\n",
    "\n",
    "news_file_train = '/home/peitian_zhang/Data/MIND/MIND'+hparams['mode']+'_train/news.tsv'\n",
    "news_file_test = '/home/peitian_zhang/Data/MIND/MIND'+hparams['mode']+'_dev/news.tsv'\n",
    "\n",
    "behavior_file_train = '/home/peitian_zhang/Data/MIND/MIND'+hparams['mode']+'_train/behaviors.tsv'\n",
    "behavior_file_test = '/home/peitian_zhang/Data/MIND/MIND'+hparams['mode']+'_dev/behaviors.tsv'\n",
    "\n",
    "save_path = '/home/peitian_zhang/Codes/NR/models/model_param/NPA_'+ hparams['mode'] +'.model'\n",
    "\n",
    "if not os.path.exists('data/dictionaries/vocab_{}_{}_{}.pkl'.format(hparams['mode'],'train','_'.join(hparams['attrs']))):\n",
    "    constructBasicDict(news_file_train,behavior_file_train,hparams['mode'],'train',hparams['attrs'])\n",
    "\n",
    "if not os.path.exists('data/dictionaries/vocab_{}_{}_{}.pkl'.format(hparams['mode'],'test','_'.join(hparams['attrs']))):\n",
    "    constructBasicDict(news_file_test,behavior_file_test,hparams['mode'],'test',hparams['attrs'])\n",
    "\n",
    "device = torch.device(hparams['gpu']) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "dataset_train = MIND_map(hparams=hparams,mode='train',news_file=news_file_train,behaviors_file=behavior_file_train)\n",
    "\n",
    "dataset_test = MIND_iter(hparams=hparams,mode='test',news_file=news_file_test,behaviors_file=behavior_file_test)\n",
    "\n",
    "vocab_train = dataset_train.vocab\n",
    "embedding = FastText('simple',cache='.vector_cache')\n",
    "vocab_train.load_vectors(embedding)\n",
    "\n",
    "vocab_test = dataset_test.vocab\n",
    "vocab_test.load_vectors(embedding)\n",
    "\n",
    "loader_train = DataLoader(dataset_train,batch_size=hparams['batch_size'],shuffle=True,pin_memory=True,num_workers=3,drop_last=True)\n",
    "loader_test = DataLoader(dataset_test,batch_size=hparams['batch_size'],pin_memory=True,num_workers=0,drop_last=True)\n",
    "\n",
    "writer = SummaryWriter('data/tb/npa/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "NPAModel(\n",
       "  (userProject): Linear(in_features=1, out_features=50, bias=True)\n",
       "  (wordQueryProject): Linear(in_features=50, out_features=200, bias=True)\n",
       "  (newsQueryProject): Linear(in_features=50, out_features=200, bias=True)\n",
       "  (wordPrefProject): Linear(in_features=200, out_features=400, bias=True)\n",
       "  (newsPrefProject): Linear(in_features=200, out_features=400, bias=True)\n",
       "  (CNN): Conv1d(300, 400, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (RELU): ReLU()\n",
       "  (DropOut): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "npaModel = NPAModel(vocab=vocab_train,hparams=hparams).to(device)\n",
    "npaModel.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training...\n",
      "epoch 0 , step 1560 , loss: 1.6223: : 1569it [05:31,  4.74it/s]\n",
      "epoch 1 , step 1560 , loss: 1.6108: : 1569it [05:27,  4.78it/s]\n",
      "epoch 2 , step 1560 , loss: 1.6145: : 1569it [05:32,  4.72it/s]\n",
      "epoch 3 , step 1560 , loss: 1.6561: : 1569it [05:32,  4.72it/s]\n",
      "epoch 4 , step 1560 , loss: 1.6090: : 1569it [05:31,  4.73it/s]\n",
      "epoch 5 , step 1560 , loss: 1.5995: : 1569it [05:29,  4.77it/s]\n",
      "epoch 6 , step 1560 , loss: 1.5960: : 1569it [05:30,  4.75it/s]\n",
      "epoch 7 , step 1560 , loss: 1.6478: : 1569it [05:27,  4.79it/s]\n",
      "epoch 8 , step 1560 , loss: 1.6660: : 1569it [05:30,  4.75it/s]\n",
      "epoch 9 , step 1560 , loss: 1.6981: : 1569it [05:29,  4.76it/s]\n"
     ]
    }
   ],
   "source": [
    "if npaModel.training:\n",
    "    print(\"training...\")\n",
    "    loss_func = getLoss(npaModel)\n",
    "    optimizer = optim.Adam(npaModel.parameters(),lr=0.001)\n",
    "    npaModel = run_train(npaModel,loader_train,optimizer,loss_func,writer, epochs=hparams['epochs'], interval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]evaluating...\n",
      "109it [00:18,  5.95it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'group_auc': 0.5138, 'ndcg@4': 0.2111, 'mean_mrr': 0.2331}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "print(\"evaluating...\")\n",
    "npaModel.eval()\n",
    "npaModel.vocab = vocab_test\n",
    "npaModel.npratio = -1\n",
    "\n",
    "run_eval(npaModel,loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}