{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37964bit4c03300bedca44f8b0013abe02048abc",
   "display_name": "Python 3.7.9 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "# pylint: skip-file\n",
    "\n",
    "\n",
    "def sinkhorn_forward(C, mu, nu, epsilon, max_iter):\n",
    "    \"\"\"standard forward of sinkhorn.\"\"\"\n",
    "\n",
    "    bs, _, k_ = C.size()\n",
    "\n",
    "    v = torch.ones([bs, 1, k_])/(k_)\n",
    "    G = torch.exp(-C/epsilon)\n",
    "    if torch.cuda.is_available():\n",
    "        v = v.cuda()\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        u = mu/(G*v).sum(-1, keepdim=True)\n",
    "        v = nu/(G*u).sum(-2, keepdim=True)\n",
    "\n",
    "    Gamma = u*G*v\n",
    "    return Gamma\n",
    "\n",
    "\n",
    "def sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter):\n",
    "    \"\"\"sinkhorn forward in log space.\"\"\"\n",
    "\n",
    "    bs, n, k_ = C.size()\n",
    "    k = k_-1\n",
    "\n",
    "    f = torch.zeros([bs, n, 1])\n",
    "    g = torch.zeros([bs, 1, k+1])\n",
    "    if torch.cuda.is_available():\n",
    "        f = f.cuda()\n",
    "        g = g.cuda()\n",
    "    epsilon_log_mu = epsilon*torch.log(mu)\n",
    "    epsilon_log_nu = epsilon*torch.log(nu)\n",
    "    def min_epsilon_row(Z, epsilon):\n",
    "        return -epsilon*torch.logsumexp((-Z)/epsilon, -1, keepdim=True)\n",
    "\n",
    "    def min_epsilon_col(Z, epsilon):\n",
    "        return -epsilon*torch.logsumexp((-Z)/epsilon, -2, keepdim=True)\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        f = min_epsilon_row(C-g, epsilon)+epsilon_log_mu\n",
    "        g = min_epsilon_col(C-f, epsilon)+epsilon_log_nu\n",
    "\n",
    "    Gamma = torch.exp((-C+f+g)/epsilon)\n",
    "    return Gamma\n",
    "\n",
    "\n",
    "def sinkhorn_backward(grad_output_Gamma, Gamma, mu, nu, epsilon):\n",
    "    nu_ = nu[:,:,:-1]\n",
    "    Gamma_ = Gamma[:,:,:-1]\n",
    "\n",
    "    bs, n, k_ = Gamma.size()\n",
    "\n",
    "    inv_mu = 1./(mu.view([1,-1]))  #[1, n]\n",
    "    Kappa = torch.diag_embed(nu_.squeeze(-2)) \\\n",
    "            -torch.matmul(Gamma_.transpose(-1, -2) * inv_mu.unsqueeze(-2), Gamma_)   #[bs, k, k]\n",
    "\n",
    "    inv_Kappa = torch.inverse(Kappa) #[bs, k, k]\n",
    "\n",
    "    Gamma_mu = inv_mu.unsqueeze(-1)*Gamma_\n",
    "    L = Gamma_mu.matmul(inv_Kappa) #[bs, n, k]\n",
    "    G1 = grad_output_Gamma * Gamma #[bs, n, k+1]\n",
    "\n",
    "    g1 = G1.sum(-1)\n",
    "    G21 = (g1*inv_mu).unsqueeze(-1)*Gamma  #[bs, n, k+1]\n",
    "    g1_L = g1.unsqueeze(-2).matmul(L)  #[bs, 1, k]\n",
    "    G22 = g1_L.matmul(Gamma_mu.transpose(-1,-2)).transpose(-1,-2)*Gamma  #[bs, n, k+1]\n",
    "    G23 = - F.pad(g1_L, pad=(0, 1), mode='constant', value=0)*Gamma  #[bs, n, k+1]\n",
    "    G2 = G21 + G22 + G23  #[bs, n, k+1]\n",
    "\n",
    "    del g1, G21, G22, G23, Gamma_mu\n",
    "\n",
    "    g2 = G1.sum(-2).unsqueeze(-1) #[bs, k+1, 1]\n",
    "    g2 = g2[:,:-1,:]  #[bs, k, 1]\n",
    "    G31 = - L.matmul(g2)*Gamma  #[bs, n, k+1]\n",
    "    G32 = F.pad(inv_Kappa.matmul(g2).transpose(-1,-2), pad=(0, 1), mode='constant', value=0)*Gamma  #[bs, n, k+1]\n",
    "    G3 = G31 + G32  #[bs, n, k+1]\n",
    "#            del g2, G31, G32, L\n",
    "\n",
    "    grad_C = (-G1+G2+G3)/epsilon  #[bs, n, k+1]\n",
    "\n",
    "    return grad_C\n",
    "\n",
    "\n",
    "class TopKFunc1(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, C, mu, nu, epsilon, max_iter):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if epsilon>1e-2:\n",
    "                Gamma = sinkhorn_forward(C, mu, nu, epsilon, max_iter)\n",
    "                if bool(torch.any(Gamma!=Gamma)):\n",
    "                    print('Nan appeared in Gamma, re-computing...')\n",
    "                    Gamma = sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter)\n",
    "            else:\n",
    "                Gamma = sinkhorn_forward_stablized(C, mu, nu, epsilon, max_iter)\n",
    "            ctx.save_for_backward(mu, nu, Gamma)\n",
    "            ctx.epsilon = epsilon\n",
    "\n",
    "        return Gamma\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output_Gamma):\n",
    "\n",
    "        epsilon = ctx.epsilon\n",
    "        mu, nu, Gamma = ctx.saved_tensors\n",
    "        # mu [1, n, 1]\n",
    "        # nu [1, 1, k+1]\n",
    "        #Gamma [bs, n, k+1]\n",
    "        with torch.no_grad():\n",
    "            grad_C = sinkhorn_backward(grad_output_Gamma, Gamma, mu, nu, epsilon)\n",
    "        return grad_C, None, None, None, None\n",
    "\n",
    "\n",
    "class TopK_custom(torch.nn.Module):\n",
    "    def __init__(self, k, epsilon=0.1, max_iter = 200):\n",
    "        super(TopK_custom, self).__init__()\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.anchors = torch.FloatTensor([0,1]).view([1,1, 2])\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.anchors = self.anchors.cuda()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        bs, n = scores.size()\n",
    "        scores = scores.view([bs, n, 1])\n",
    "\n",
    "        #find the -inf value and replace it with the minimum value except -inf\n",
    "        scores_ = scores.clone().detach()\n",
    "        max_scores = torch.max(scores_).detach()\n",
    "        scores_[scores_==float('-inf')] = float('inf')\n",
    "        min_scores = torch.min(scores_).detach()\n",
    "        filled_value = min_scores - (max_scores-min_scores)\n",
    "        mask = scores==float('-inf')\n",
    "        scores = scores.masked_fill(mask, filled_value)\n",
    "\n",
    "        C = (scores-self.anchors)**2\n",
    "        C = C / (C.max().detach())\n",
    "        #print(C)\n",
    "        mu = torch.ones([1, n, 1], requires_grad=False)/n\n",
    "        nu = torch.FloatTensor([self.k/n, (n-self.k)/n]).view([1, 1, 2])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            mu = mu.cuda()\n",
    "            nu = nu.cuda()\n",
    "\n",
    "        Gamma = TopKFunc1.apply(C, mu, nu, self.epsilon, self.max_iter)\n",
    "        #print(Gamma)\n",
    "        A = Gamma[:,:,0]*n\n",
    "\n",
    "        return A\n",
    "\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "class TopK_stablized(torch.nn.Module):\n",
    "    def __init__(self, k, epsilon=0.1, max_iter = 200):\n",
    "        super(TopK_stablized, self).__init__()\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.anchors = torch.FloatTensor([0,1]).view([1,2,1])\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.anchors = self.anchors.cuda()\n",
    "\n",
    "    def forward(self, scores):\n",
    "        bs, n = scores.size()[:2]\n",
    "        scores = scores.view([bs, 1, n])\n",
    "\n",
    "        #find the -inf value and replace it with the minimum value except -inf\n",
    "        scores_ = scores.clone().detach()\n",
    "        max_scores = torch.max(scores_).detach()\n",
    "        scores_[scores_==float('-inf')] = float('inf')\n",
    "        min_scores = torch.min(scores_).detach()\n",
    "        filled_value = min_scores - (max_scores-min_scores)\n",
    "        mask = scores==float('-inf')\n",
    "        scores = scores.masked_fill(mask, filled_value)\n",
    "\n",
    "        C = (scores-self.anchors)**2\n",
    "        C = C / (C.max().detach())\n",
    "        f = torch.zeros([bs, 1, n])\n",
    "        g = torch.zeros([bs, 2, 1])\n",
    "        mu = torch.ones([1, 1, n], requires_grad=False)/n\n",
    "        nu = torch.FloatTensor([self.k/n, (n-self.k)/n]).view([1, 2, 1])\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            f = f.cuda()\n",
    "            g = g.cuda()\n",
    "            mu = mu.cuda()\n",
    "            nu = nu.cuda()\n",
    "\n",
    "        def min_epsilon_row(Z, epsilon):\n",
    "            return -epsilon*torch.logsumexp((-Z)/epsilon, -1, keepdim=True)\n",
    "\n",
    "\n",
    "        def min_epsilon_col(Z, epsilon):\n",
    "            return -epsilon*torch.logsumexp((-Z)/epsilon, -2, keepdim=True)\n",
    "\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            f = min_epsilon_col(C-f-g, self.epsilon)+f+self.epsilon*torch.log(mu)\n",
    "            g = min_epsilon_row(C-f-g, self.epsilon)+ g +self.epsilon*torch.log(nu)\n",
    "\n",
    "        P = torch.exp((-C+f+g)/self.epsilon)\n",
    "        A = P[:,0,:]*n\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}